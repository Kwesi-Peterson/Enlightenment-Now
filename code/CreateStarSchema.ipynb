{"cells":[{"cell_type":"code","source":["%run ./loadData"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54512bb1-b92b-4eb2-b56c-8b2d7967cdad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import BooleanType, DoubleType, IntegerType, StringType, StructField, StructType\nfrom pyspark.sql.functions import expr, lit, regexp_replace, when\ndbutils.fs.ls(\"/mnt/worldbankdata\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74693857-a081-48fd-88d1-6b486204a15c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Get the Life Expectancy CSV and show some sample data.\ndfLifeExp = spark.read.option(\"header\", \"true\").csv(lifeExpCsv)\ndfLifeExp.select(dfLifeExp.columns[0:5]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48431512-34f7-4e3c-8708-a1392c526313"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Create the first dimension table for countries.\n# Some of the entries are regions rather than countries - add 2 columns to table to reflect this.\nfrom pyspark.sql.functions import lit\ndimCountrySchema = StructType([ \\\n                               StructField(\"regionCode\", StringType(), True), \\\n                               StructField(\"countryName\", StringType(), True), \\\n                               StructField(\"isCountry\", BooleanType(), True), \\\n                               StructField(\"isRegion\", BooleanType(), True), \\\n                              ])\ncountriesSet = {\"ABW\", \"AFG\", \"AGO\", \"ALB\", \"AND\", \"ARE\", \"ARG\", \"ARM\", \"ASM\", \"ATG\",\n                \"AUS\", \"AUT\", \"AZE\", \"BDI\", \"BEL\", \"BEN\", \"BFA\", \"BGD\", \"BGR\", \"BHR\",\n                \"BHS\", \"BIH\", \"BLR\", \"BLZ\", \"BMU\", \"BOL\", \"BRA\", \"BRB\", \"BRN\", \"BTN\", \n                \"BWA\", \"CAF\", \"CAN\", \"CHE\", \"CHI\", \"CHL\", \"CHN\", \"CIV\", \"CMR\", \"COD\",\n                \"COG\", \"COL\", \"COM\", \"CPV\", \"CRI\", \"CUB\", \"CUW\", \"CYM\", \"CYP\", \"CZE\",\n                \"DEU\", \"DJI\", \"DMA\", \"DNK\", \"DOM\", \"DZA\", \"ECU\", \"EGY\", \"ERI\", \"ESP\",\n                \"EST\", \"ETH\", \"FIN\", \"FJI\", \"FRA\", \"FRO\", \"FSM\", \"GAB\", \"GBR\", \"GEO\",\n                \"GHA\", \"GIB\", \"GIN\", \"GMB\", \"GNB\", \"GNQ\", \"GRC\", \"GRD\", \"GRL\", \"GTM\",\n                \"GUM\", \"GUY\", \"HKG\", \"HND\", \"HRV\", \"HTI\", \"HUN\", \"IDN\", \"IMN\", \"IND\",\n                \"IRL\", \"IRN\", \"IRQ\", \"ISL\", \"ISR\", \"ITA\", \"JAM\", \"JOR\", \"JPN\", \"KAZ\",\n                \"KEN\", \"KGZ\", \"KHM\", \"KIR\", \"KNA\", \"KOR\", \"KWT\", \"LAO\", \"LBN\", \"LBR\",\n                \"LBY\", \"LCA\", \"LIE\", \"LKA\", \"LSO\", \"LTU\", \"LUX\", \"LVA\", \"MAC\", \"MAF\",\n                \"MAR\", \"MCO\", \"MDA\", \"MDG\", \"MDV\", \"MEX\", \"MHL\", \"MKD\", \"MLI\", \"MLT\",\n                \"MMR\", \"MNE\", \"MNG\", \"MNP\", \"MOZ\", \"MRT\", \"MUS\", \"MWI\", \"MYS\", \"NAM\",\n                \"NCL\", \"NER\", \"NGA\", \"NIC\", \"NLD\", \"NOR\", \"NPL\", \"NRU\", \"NZL\", \"OMN\",\n                \"PAK\", \"PAN\", \"PER\", \"PHL\", \"PLW\", \"PNG\", \"POL\", \"PRI\", \"PRK\", \"PRT\",\n                \"PRY\", \"PSE\", \"PYF\", \"QAT\", \"ROU\", \"RUS\", \"RWA\", \"SAU\", \"SDN\", \"SEN\",\n                \"SGP\", \"SLB\", \"SLE\", \"SLV\", \"SMR\", \"SOM\", \"SRB\", \"SSD\", \"STP\", \"SUR\",\n                \"SVN\", \"SVN\", \"SWE\", \"SWZ\", \"SXM\", \"SYC\", \"SYC\", \"TCA\", \"TCD\", \"TGO\",\n                \"THA\", \"TJK\", \"TKM\", \"TLS\", \"TON\", \"TTO\", \"TUN\", \"TUR\", \"TUV\", \"TZA\",\n                \"UGA\", \"UKR\", \"URY\", \"USA\", \"UZB\", \"VCT\", \"VEN\", \"VGB\", \"VIR\", \"VNM\",\n                \"VUT\", \"WSM\", \"XKX\", \"YEM\", \"ZAF\", \"ZMB\", \"ZWE\"}\n\nregionSet = {\"AFE\", \"AFW\", \"ARB\", \"CEB\", \"CSS\", \"EAS\", \"ECS\", \"EMU\", \"EUU\", \"LCN\",\n             \"MEA\", \"NAC\", \"SAS\", \"SSF\"}\n\ndimCountry = dfLifeExp.select(col(\"Country Code\").alias(\"regionCode\"), col(\"Country Name\").alias(\"countryName\"))\ndimCountry = dimCountry.withColumn(\"isCountry\", when(dimCountry.regionCode.isin(countriesSet), True).otherwise(None)).withColumn(\"isRegion\", when(dimCountry.regionCode.isin(regionSet), True).otherwise(None))\ndisplay(dimCountry)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c155088-ee98-49f4-9dbc-d8bf59d5123d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Create the second dimension table for indicator codes.\ndimIndicatorSchema = StructType([ \\\n                                 StructField(\"indicatorCode\", StringType(), True), \\\n                                 StructField(\"indicatorName\", StringType(), True), \\\n                                ])\ndimIndicatorData = []\n\nfor csv in csvList:\n    df = spark.read.option(\"header\", \"true\").csv(csv)\n    indicatorCode = df.select(col(\"Indicator Code\")).collect()[0][0]\n    indicatorName = df.select(col(\"Indicator Name\")).collect()[0][0]\n    tuple = (indicatorCode, indicatorName)\n    dimIndicatorData.append(tuple)\n\ndimIndicator = spark.createDataFrame(data=dimIndicatorData, schema=dimIndicatorSchema)\ndisplay(dimIndicator)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa9369a8-8b98-42c2-8344-b3e3e7a205c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Create the third dimension table for dates\ndimDateSchema = StructType([ \\\n                            StructField(\"dateYear\", IntegerType(), True), \\\n                            StructField(\"dateDecade\", StringType(), True), \\\n                           ])\ndimDateData = []\n\nfor year in range(1960, 2021):\n    if 1960 <= year < 1970:\n        tuple = (year, \"1960s\")\n    elif 1970 <= year < 1980:\n        tuple = (year, \"1970s\")\n    elif 1980 <= year < 1990:\n        tuple = (year, \"1980s\")\n    elif 1990 <= year < 2000:\n        tuple = (year, \"1990s\")\n    elif 2000 <= year < 2010:\n        tuple = (year, \"2000s\")\n    elif 2010 <= year < 2020:\n        tuple = (year, \"2010s\")\n    else:\n        tuple = (year, \"2020s\")\n    dimDateData.append(tuple)\n\ndimDate = spark.createDataFrame(data=dimDateData, schema=dimDateSchema)\ndisplay(dimDate)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7237ea87-d9dd-4a4e-82c6-b5bdd7a1f47f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Next, create the fact table.\nfrom pyspark.sql.types import BooleanType, DoubleType, IntegerType, StringType, StructField, StructType\nfactSchema = StructType([ \\\n                            StructField(\"regionCode\", StringType(), True), \\\n                            StructField(\"indicatorCode\", StringType(), True), \\\n                            StructField(\"dateYear\", IntegerType(), True), \\\n                            StructField(\"value\", DoubleType(), True), \\\n                           ])\nfactData = []\n\nfor csv in csvList:\n    df = spark.read.option(\"header\", \"true\").csv(csv)\n    numCodes = df.select(col(\"Country Code\")).count()\n    indicatorCode = df.select(col(\"Indicator Code\")).collect()[0][0]\n    regionColumn = df.select(col(\"Country Code\")).collect()\n    for year in range(1960, 2021):\n        yearStr = str(year)\n        yearColumn = df.select(col(yearStr)).collect()\n        for counter in range(0, numCodes):\n            regionCode = regionColumn[counter].asDict()['Country Code']\n            value = yearColumn[counter].asDict()[yearStr]\n            if value is not None:\n                valueFloat = float(value)\n                tuple = (regionCode, indicatorCode, year, valueFloat)\n            else:\n                tuple = (regionCode, indicatorCode, year, value)\n            factData.append(tuple)\n\nfactTable = spark.createDataFrame(data=factData, schema=factSchema)\nfactTable.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5df7aed7-fbf5-4a0f-880a-b7b5e3881178"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Try creating the fact table by unpivoting. to see if we get better performance\nfactSchema = StructType([ \\\n                            StructField(\"regionCode\", StringType(), True), \\\n                            StructField(\"indicatorCode\", StringType(), True), \\\n                            StructField(\"dateYear\", IntegerType(), True), \\\n                            StructField(\"value\", DoubleType(), True), \\\n                           ])\n\nfactTable2 = spark.createDataFrame(data=[], schema=factSchema)\nfor csv in csvList:\n    ingestedDf = spark.read.option(\"header\", \"true\").csv(csv)\n    factDf = ingestedDf.select(col(\"Country Code\"), col(\"Indicator Code\"), lit(ingestedDf.columns[4]).alias(\"dateYear\"),ingestedDf[ingestedDf.columns[4]].alias(\"value\"))\n    for counter in ingestedDf.columns[5:]:\n        factDf = factDf.union(ingestedDf.select(col(\"Country Code\"), col(\"Indicator Code\"), lit(counter), ingestedDf[counter]))\n    factDf = factDf.withColumnRenamed(\"Country Code\", \"regionCode\").withColumnRenamed(\"Indicator Code\", \"indicatorCode\")\n    factTable2 = factTable.union(factDf)\n\nfactTable2 = factTable2.withColumn(\"value\", col(\"value\").cast('double')).withColumn(\"dateYear\", col(\"dateYear\").cast('integer'))\ndisplay(factTable2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05e9cb5c-d14b-4941-a3a0-03efd1bf43d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Count the number of rows in the tables\ndimCountry.count()\ndimIndicator.count()\ndimDate.count()\nfactTable.count()\nfactTable2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f7dfde3-2249-49a0-b1bd-5f28cf8e6261"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Finally, save the tables! Because factTable2 is significantly faster than factTable1, we pick factTable2 as our primary Fact table.\ndimCountry.write.mode(\"overwrite\").saveAsTable(\"DimensionTableCountryCode\")\ndimIndicator.write.mode(\"overwrite\").saveAsTable(\"DimensionTableIndicatorCode\")\ndimDate.write.mode(\"overwrite\").saveAsTable(\"DimensionTableDate\")\nfactTable.write.mode(\"overwrite\").saveAsTable(\"FactTableBackup\")\nfactTable2.write.mode(\"overwrite\").saveAsTable(\"FactTable\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7fa4a47-5c81-45e1-a153-793b904c3e38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e39df30-bd58-4cf2-a39e-aa16c5f81947"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CreateStarSchema","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3429016939827111}},"nbformat":4,"nbformat_minor":0}
